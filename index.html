
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="DDDM-VC project page">
    <meta name="keywords" content="DDDMs, Diffusion models, Voice Conversion, Text-To-Speech, Audio Mixing">
    <title>DDDM-VC</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link href="css/bootstrap.css" rel="stylesheet">
    <link href="css/main.css" rel="stylesheet">
    <script src="https://code.jquery.com/jquery-1.10.2.min.js" type="text/javascript"></script>
    <script src="js/hover.zoom.js" type="text/javascript"></script>
    <script src="js/hover.zoom.conf.js" type="text/javascript""></script>

    <link rel="stylesheet" href="css/bulma.min.css">
    <link rel="stylesheet" href="css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <!-- <script defer src="js/fontawesome.all.min.js" type="text/javascript"></script> -->
    <script defer src="js/fontawesome.all.min.js"></script>
  </head>
  
  <body>
    <div id="white">
      <div class="container">
        <div class="row">
          <div class="col-lg-12  centered">
            <span style="font-weight:bold; font-size:40px">
              DDDM-VC: Decoupled Denoising Diffusion Models with Disentangled Representation and Prior Mixup for Verified Robust Voice Conversion
            </span><br>
  
        <div class="row">
          <div class="col-lg-1 ">
            <center></center>
            </div>
        </div><br>

        <div class="row">
          <div class="col-lg-1 col-lg-offset-2 centered">
            <center></center>
          </div>
        </div><br>

        <div class="row">
          <div class="col-lg-4 col-lg-offset-4 centered">
            <span class="link-block">
              <a href="" class="mybutton button">
              <span class="gradient"></span>
              <span class="icon"><i class="fab fa-github"></i>
              <span>Code</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://dddm-vc.github.io/demo/" class="mybutton button">
              <span class="gradient"></span>
              <span class="icon"><i class="fa fa-headphones" aria-hidden="true"></i>
              <span>Audio Demo</span>
              </a>
            </span>
        </div></div>
      
      <div class="row"><br>
        <strong>üéß Our audio samples are available at above Audio Demo site.<br></strong>
          üñ•Ô∏è Source code and model checkpoint will be released after the paper decision. 
        
      </div>
      <br>

      <div class="row">
        <div class="col-lg-1 ">
          <center></center>
          </div>
      </div><br>

    <div class="row">
      <center>
      <span style="font-weight: bold;font-size:35px">Abstract</span>
        <hr width="100%">
        <div style="text-align:justify;font-size:30px">
          <p>
            Diffusion-based generative models have exhibited powerful generative performance in recent years. 
            However, as many attributes exist in the data distribution and owing to several limitations of sharing the model parameters across all levels of the generation process, 
            it remains challenging to control specific styles for each attribute. 
            To address the above problem, this paper presents decoupled denoising diffusion models (DDDMs) with disentangled representations, which can control the style for each attribute in generative models. 
            We apply DDDMs to voice conversion (VC) tasks to address the challenges of disentangling and controlling each speech attribute (e.g., linguistic information, intonation, and timbre). 
            First, we use a self-supervised representation to disentangle the speech representation. 
            Subsequently, the DDDMs are applied to resynthesize the speech from the disentangled representations for denoising with respect to each attribute. 
            Moreover, we also propose the prior mixup for robust voice style transfer, which uses the converted representation of the mixed style as a prior distribution for the diffusion models. 
            The experimental results reveal that our method outperforms publicly available VC models. 
            Furthermore, we show that our method provides robust generative performance regardless of the model size. 
        </div>
        </center>
    </div><br><br>

<div class="row">
  <span style="font-weight: bold;font-size:35px">Model Architecture</span>
  <hr width="100%"></center>
  <div class="col-lg-10 col-lg-offset-1 centered">
    <img src="assets/dddm_vc_project.png" width="100%" alt="Responsive image">
</div></div>
<br> <em>Overall framework of DDDM-VC</em><br><br>

<section class="section" style="background-color:#8b8bc981">
    <div class="columns is-centered has-text-centered">
      <div style="width:100%;">
        <div style="float:left; width:45%;">
          <p style="font-size:25px">Filter Denoiser</p>
          <div class="content has-text-justified">
            <img src="assets/ftr_dec.gif" style="max-width: 100%; height: 100%;"> 
          </div>
          <p> Filter attribute denoising process </p>  
        </div>
        <div style="float:right; width:45%;">
          <p style="font-size:25px">Source Denoiser</p> 
          <div class="content has-text-justified">
            <img src="assets/src_dec.gif" style="max-width: 100%; height: 100%;"> 
          </div> 
          <p> Source attribute denoising process </p>
        </div>
      </div>  
    </div>     
</section>

<div class="row">
  <div class="col-lg-1 ">
    <center></center>
    </div>
</div><br>  
<div class="row">
  <span style="font-weight: bold;font-size:35px">Other application of DDDMs</span>
  <hr width="100%"></center>
  <div class="col-lg-10 col-lg-offset-1 centered"> 
</div></div>
 
 
<div class="row">
  <div class="col-lg-1 ">
    <center></center>
    </div>
</div><br>  
<div class="row">
  <span style="font-weight: 600;font-size:35px"><a href="https://dddm-vc.github.io/demo/#dddm_tts" target="_blank"  style="color: #DA206D">DDDM-TTS</a></span>
  <hr width="100%"></center>
  <div class="col-lg-10 col-lg-offset-1 centered">
    <img src="assets/dddm_tts.png" width="100%" alt="Responsive image">
</div></div> 
<br><em>Overall framework of DDDM-TTS</em><br><br>
<div style="text-align:justify;font-size:30px">
<p>
  For a practical application, we have experimented with an extension version of DDDM-VC for a text-to-speech system.
  In DDDM-VC, our goal is to train the model without any text transcripts, and only utilize the self-supervised representation for speech disentanglement. 
  Based on the DDDM-VC, we train the text-to-vec (TTV) model which can generate the self-supervised speech representation 
  (the representation from the middle layer of XLS-R) from the text as a content representation. 
    We jointly train the duration predictor and pitch predictor. The predicted content and pitch representation 
    are fed to DDDM-VC instead of each representation from the waveform to synthesize the speech. Hence, we could synthesize 
    the speech from text by utilizing the pre-trained DDDM-VC. 
</div>


 
<div class="row">
  <div class="col-lg-1 ">
    <center></center>
    </div>
</div><br>  
<div class="row">
  <span style="font-weight: 600;font-size:35px"><a href="https://dddm-vc.github.io/demo/#audio_mixer" target="_blank" style="color: #DA206D">DDDM-Mixer</a></span>
  <hr width="100%"></center>
  <div class="col-lg-10 col-lg-offset-1 centered">
    <img src="assets/dddm_mixer.png" width="100%" alt="Responsive image">
</div></div>
<br><em>Overall framework of DDDM-Mixer</em><br><br>
<div style="text-align:justify;font-size:30px">
<p>
  We extend DDDMs to DDDM-Mixer which leverages multiple denoisers to mix the sound and speech into 
  the mixture of Mel-spectrograms by blending them with the desired balance. 
  While DDDM-VC disentangles a single speech into source and filter attributes for attribute denoisers, 
  DDDM-Mixer treats the mixture of audio as a target audio and disentangles it into sound and speech as attributes. 
  We utilize the data augmentation according to signal-to-noise ratio (SNR) from -5 dB to 25 dB for target audio. 
  DDDM-Mixer utilizes the Mel-spectrogram of each attribute as a prior. 
  We employ two denoisers, both of which remove a single noise in terms of their own attribute (sound and speech). 
  We utilize an SNR dB between sound and speech as conditional information to mix each attribute with a desired ratio. 
  We concatenate the SNR positional embedding with the time positional embedding for the condition. 
  Hence, we could mix the generated sound and speech on the Mel-spectrogram. 
  To verify the effectiveness of the DDDM-Mixer, we compare the augmented audio between vocoded sound and speech 
  and the vocoded audio from mixed Mel-spectrogram. 
  The evaluation results show DDDM-Mixer has better performance than audio augmentation according to SNR between the vocoded sound and speech.  
</div>



<div class="row">
  <div class="col-lg-4 ">
    <center></center>
    </div>
</div><br>
<div class="row">
  <div class="col-lg-4 ">
    <center></center>
    </div>
</div><br>




<script src="../js/bootstrap.min.js" type="text/javascript"></script>

  <style type="text/css">
    .layered-paper-big {
      box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        5px 5px 0 0px #fff,
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        10px 10px 0 0px #fff,
        10px 10px 1px 1px rgba(0, 0, 0, 0.35),
        15px 15px 0 0px #fff,
        15px 15px 1px 1px rgba(0, 0, 0, 0.35),
        20px 20px 0 0px #fff,
        20px 20px 1px 1px rgba(0, 0, 0, 0.35),
        25px 25px 0 0px #fff,
        25px 25px 1px 1px rgba(0, 0, 0, 0.35);
      margin-left: 10px;
      margin-right: 45px;
    }

    .layered-paper {
      box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        5px 5px 0 0px #fff,
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        10px 10px 0 0px #fff,
        10px 10px 1px 1px rgba(0, 0, 0, 0.35);
      margin-top: 5px;
      margin-left: 10px;
      margin-right: 30px;
      margin-bottom: 5px;
    }
  </style>

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: { inlineMath: [['$$', '$$']], displayMath: [['$$$', '$$$']] }
        });
    </script>
    <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
<script src="/cdn-cgi/scripts/7d0fa10a/cloudflare-static/rocket-loader.min.js" data-cf-settings="d5da7a36741738a7ea3dbd4d-|49" defer=""></script></body>
</html>